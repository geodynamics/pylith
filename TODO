======================================================================
MAIN PRIORITIES (Brad)
======================================================================

Cleanup

  Add check at beginning of simulation to make sure cell dimension
  matches space dimension. We don't have enough information to
  determine the fault orientation information for a cell in a higher
  dimension coordinate space. We also haven't implemented the
  elasticity stuff for lower dimension cells in a higher
  dimensions. Since Sieve can handle lower order meshes in higher
  dimensions, we probably want to allow it in MeshIO, just not in the
  simulation where we don't support it.

  Use pylith/utils/sievefwd.hh and pylith/utils/sievetypes.hh in unit
  tests. Remove typedefs when possible (take advantage of typedefs in
  seivefwd.hh).

  Use MeshIOAscii to get mesh information into Python tests?

0. Implement faults for kinematic source
   a. Add creation of cohesive cells
     i. Add tests for interpolated meshes and constrained cohesive cells.
        Double check consistency in ordering of vertices (positive/negative).

   b. Implement slip time function
     ii. Add Python
     iii. Add unit tests

   c. Start implementing integrator for faults

     i. Update Reference cell
       (1) Compute basis and basisDeriv at vertices
       (2) Add unit tests

     ii. Update Quadrature
       (1) Add basis and basisDeriv at vertices to Quadrature
       (2) Add computeGeometryQuad/computeGeometryVert to Quadrature
       (3) Add unit tests

     iii. FaultCohesive
       (1) Unit tests

     iv. FaultCohesiveKin
       (1) initialize(), integrateResidual(), integrateJacobian()
       (2) Add Python unit tests
       (3) Add C++ unit tests

1. Finish implementing ExplicitElasticity and Explicit
   a. Replace integrateConstant() with integrateResidual()

     {f}-[A]{u} {u} is "guess" (zero for explicit)

   a. Double check loops for integrateResidual() and integrateJacobian()
   b. Create unit test (construction of residual term and Jacobian)

   c. Add unit test for IntegratorElasticity::calcTotalStrain

2. Implement HDF5 output

3. Implement PML boundary conditions

4. Create suite of simple full test cases

======================================================================
SECONDARY PRIORITIES
======================================================================

1. Implement MeshIOCubit
   a. C++ objects
   b. unit tests at C++ level
   c. Python object (MeshIOCubit)
   d. bindings
   e. unit tests at Python level

2. Implement MeshIOLagrit
   a. C++ objects
   b. unit tests at C++ level
   c. Python object (MeshIOLagrit)
   d. bindings
   e. unit tests at Python level

3. Implement MeshIOHDF5 & HDF5 (helper class)
   a. C++ objects
   b. unit tests at C++ level
   c. Python object (MeshIOHDF5)
   d. bindings
   e. unit tests at Python level

======================================================================
UNRESOLVED ISSUES
======================================================================

1. Integration of nemesis (pylithic.py as mpi/python application) [Leif?]

======================================================================
THINGS WE NEED SIEVE TO DO (Matt)
======================================================================

1. Create PETSc mesh with associated boundary condition meshes.

  In general we can only rely on mesh generators to provide vertices,
  cells, attributes for vertices and cells (i.e., one value for each
  vertex or cell), and groups of vertices and cells. As a result,
  boundary conditions would be associated with groups of
  vertices. This is nonunique if we require users to not allow
  multiple surfaces be associated with a single group of vertices. In
  other words, there is a unique set of faces for a group of
  vertices. For our boundary conditions we will want the N-1 dimension
  (where the mesh is N dimensions) representation of a subset of the
  mesh (i.e., 2-D mesh of fault surface).

  Inputs:
    * PETSc mesh
    * Group of vertices
  Outputs:
    * PETSc mesh with bc mesh

  Note: Above would be repeated for each group of vertices.

2. Create cohesive cells.

  Inputs:
    * PETSc mesh
    * Surface meshes defining faults
  Output:
    * PETSc mesh with cohesive cells.

  Note: Not sure whether this would be done with global or distributed mesh.

3. Distribute mesh/bc in scalable manner. 

  Inputs:
    * PETSc Mesh (with bc) (global on each processor)
    * number of processors
  Outputs:
    * PETSc Mesh on each processor (restricted to processor)

  [DONE]

4. Global refinement of mesh.

  Inputs:
    * PETSc Mesh (original)
    * refinement factor (limited to factor of 2?)
  Outputs:
    * PETSc Mesh (refined)

5. Construct mesh with higher order cells from mesh with lower order cells.

  Many mesh generators do not know how to construct higher order
  elements, so we will need a general utility for doing this.

  Inputs:
    * PETSc Mesh
    * some sort of map (Python object) defining how to construct
      higher order reference cell from lower order reference cell.
  Output:
    * PETSc Mesh

  Note: I think this would give us incredible flexibility in selecting
  the appropriate discretization for a problem. For example, I think
  it would allow us to use spectral elements.

6. Trial run with quadratic elements

7. Trial run with fully interpolated mesh

======================================================================
QUESTIONS FOR LEIF
======================================================================

How do we trap C++ exceptions in Pyrex?

